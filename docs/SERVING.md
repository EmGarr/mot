# Serving

## Object detection

In order to serve the object detection algorithm we use the native tool from tensorflow: [tensorflow serving](https://www.tensorflow.org/tfx/serving/docker).

- File structures

Whenever you serve a model you need to respect a certain hierarchy.

```
models/ # tensorflow serving can serve multiple models you'll need the folder as base folder
models/name_of_your_model/ # You can put whatever you want and path it with -e MODEL_NAME=name_of_your_model
models/name_of_your_model/1/ # 1 is the version of your model
models/name_of_your_model/1/my_model.pb # Autogenerated whenever you export for serving
models/name_of_your_model/1/variables/my_model.data.XXX # Autogenerated whenever you export for serving
models/name_of_your_model/1/variables/my_model.index # Autogenerated whenever you export for serving
```

These files are generated automatically whenever you export your model for serving.

- Run the server

Create a folder and run the following commands

```
export MODEL_PATH=`pwd` 

wget https://files.heuritech.com/raw_files/surfrider/resnet50_fpn/model.zip # a model that we haver already exported
unzip model.zip

docker run -t --rm -p 8999:8501 --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=0 \
      -v $MODEL_PATH:/models/ \ 
      tensorflow/serving:1.15.0-gpu 
```

It will: download a model and serve it.

- Query the server

```python
import cv2
import numpy as np

from urllib.request import urlopen
from mot.object_detection.query_server import localizer_tensorflow_serving_inference


response = urlopen('https://media.al.com/live/photo/11164145-large.jpg')
image = np.asarray(bytearray(response.read()), dtype="uint8")
image = np.asarray(cv2.imdecode(image, cv2.IMREAD_COLOR), dtype=np.float32)
predictions = localizer_tensorflow_serving_inference(image, 'http://localhost:8999')
```
